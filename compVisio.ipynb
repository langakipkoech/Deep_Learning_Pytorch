{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and test data\n",
    "\n",
    "training_data = torchvision.datasets.FashionMNIST( root=\"data\",\n",
    "                                                  train = True,\n",
    "                                                  download=True,\n",
    "                                                  transform=ToTensor(),\n",
    "                                                  target_transform= None\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(root=\"data\",\n",
    "                                 train = False,\n",
    "                                 download=True,\n",
    "                                 transform = ToTensor(),\n",
    "                                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the returned data, we know data type is tuple\n",
    "#returns image, index\n",
    "\n",
    "image, label = training_data[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(figsize=(6,7))\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(f\"{label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = training_data.classes\n",
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple image plots\n",
    "\n",
    "fig = plt.figure()\n",
    "#number of rows and cols\n",
    "rows, cols = 4, 4\n",
    "\n",
    "for i in range(1, rows*cols +1):\n",
    "    random_img = torch.randint(0, len(training_data), size=[1]).item()\n",
    "    img, label = training_data[random_img]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze())\n",
    "    plt.axis(False)\n",
    "    plt.title(class_name[label])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training batch: 1875 and test data batch are: 313\n"
     ]
    }
   ],
   "source": [
    "#Dataloader breaking the data to batches for computationa√∂ efficiency\n",
    "#general mutliples of twos are preferred\n",
    "\n",
    "training_data_batch = DataLoader(training_data,\n",
    "                                 batch_size=32,\n",
    "                                 shuffle=True)\n",
    "\n",
    "test_data_batch = DataLoader(test_data,\n",
    "                             batch_size=32,\n",
    "                             shuffle=True)\n",
    "\n",
    "print(f\"The length of the training batch: {len(training_data_batch)} and test data batch are: {len(test_data_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the properties of the training batch\n",
    "\n",
    "training_data_features, training_data_labels = next(iter(training_data_batch))\n",
    "training_data_features.shape, training_data_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some of the data from the batches\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "rows, cols = 4,4\n",
    "\n",
    "for i in range(1, rows*cols+1):\n",
    "    random_idx = torch.randint(0, len(training_data_features), size=[1]).item()\n",
    "    random_img_idx, random_label_idx = training_data_features[random_idx], training_data_labels[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(random_img_idx.squeeze(), cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(f\"{class_name[random_label_idx]}\")\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_layer: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_one = nn.Sequential(\n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(in_features=input_shape, out_features=hidden_layer),\n",
    "                                        nn.Linear(in_features=hidden_layer, out_features=output_shape),                                        \n",
    "        )\n",
    "\n",
    "    ##define forward method\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_one(x)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (layer_one): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create object of the class\n",
    "model_0 = FashionMNISTModel(784, 10, len(class_name))\n",
    "\n",
    "model_0.to('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimitization metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      " The files already exist now...Skipping download..\n"
     ]
    }
   ],
   "source": [
    "#check validity of url\n",
    "\n",
    "try:\n",
    "\n",
    "    r = requests.get('https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py')\n",
    "    print(f\"{r.status_code}\")\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        if Path('helper_functions.py').is_file():\n",
    "            print(f\" The files already exist now...Skipping download..\")\n",
    "\n",
    "        else:\n",
    "            with open('helper_functions.py', 'wb') as file:\n",
    "                print('files are being downloaded')\n",
    "\n",
    "            file.write(r.content)\n",
    "except Exception as e:\n",
    "    print(\" Unkown error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data in our case is in batches and we need two loops , one for epochs and the other one for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm for visualization of the progressbar as we train the model\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07ddb5d61134603a9f6a07059bc6ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the epoch: 0\n",
      "The amount of data processed is 0\n",
      "The amount of data processed is 22400\n",
      "The amount of data processed is 44800\n",
      "This is the epoch: 1\n",
      "The amount of data processed is 0\n",
      "The amount of data processed is 22400\n",
      "The amount of data processed is 44800\n",
      "This is the epoch: 2\n",
      "The amount of data processed is 0\n",
      "The amount of data processed is 22400\n",
      "The amount of data processed is 44800\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    print(f\"This is the epoch: {epoch}\")\n",
    "\n",
    "    training_loss = 0\n",
    "\n",
    "    for Batch, (X,Y) in enumerate(training_data_batch):\n",
    "\n",
    "        model_0.train()\n",
    "        \n",
    "        #forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        #calculate the loss\n",
    "\n",
    "        pred_loss = loss_fn(y_pred, Y)\n",
    "\n",
    "        training_loss += pred_loss\n",
    "\n",
    "        #optimizer and zero grad\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #step backwards\n",
    "        pred_loss.backward()\n",
    "\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        if Batch % 700 == 0:\n",
    "            print(f\"The amount of data processed is {Batch*len(X)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
